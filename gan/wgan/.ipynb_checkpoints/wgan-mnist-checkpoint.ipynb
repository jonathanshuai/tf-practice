{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7503459608064888293\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6709723791\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16707502147463884149\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CROPS = False # Determine whether to run crops or not\n",
    "\n",
    "HEIGHT = 28 # Height of the image\n",
    "WIDTH = 28 # Width of the image\n",
    "CHANNELS = 1 # Number of channels for an image\n",
    "\n",
    "Z_DIM = 100 # Dimensions of noise vector z\n",
    "BATCH_SIZE = 64 # Batch size for training\n",
    "\n",
    "G_FILTERS = 32 # Number of filters in the final deconv layer for Generator\n",
    "D_FILTERS = 32 # Number of filters in the first conv layer for Discriminator\n",
    "\n",
    "WEIGHT_CLIP = 0.01\n",
    "D_LEARNING_RATE = 1.5e-4\n",
    "G_LEARNING_RATE = 1.5e-4\n",
    "N_EPOCHS = 50\n",
    "BETA1 = 0.5\n",
    "G_ITERS = 4\n",
    "G_USE_BATCHNORM = True\n",
    "D_USE_BATCHNORM = True\n",
    "\n",
    "TIME_FORMAT = datetime.now().strftime('%h-%d-%Y-%Hh%Mm%Ss')\n",
    "\n",
    "g_filters = [256, 128, 64]\n",
    "g_kernels = [5, 5, 5]\n",
    "g_strides = [1, 2, 2]\n",
    "\n",
    "d_filters = [64, 128, 256]\n",
    "d_kernels = [5, 5, 5]\n",
    "d_strides = [2, 2, 1]\n",
    "RUN_NAME = f\"d_lr_{D_LEARNING_RATE}_g_lr{G_LEARNING_RATE}_{BETA1}_{G_ITERS}_{N_EPOCHS}_dbn_{D_USE_BATCHNORM}_gbn_{G_USE_BATCHNORM}_{g_filters[0]}_{g_kernels[0]}_{g_strides[0]}_{TIME_FORMAT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_dir = 'C:\\\\Users\\\\Jonathan\\\\Documents\\\\development\\\\datasets\\\\images\\\\mnist\\\\training'\n",
    "image_list = []\n",
    "\n",
    "for directory in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "    for root, d, files in os.walk(os.path.join(image_dir, directory)):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                image_list.append(os.path.join(directory, file))\n",
    "\n",
    "plt.imshow(cv2.imread(os.path.join(image_dir, image_list[0])))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list = image_list[:59904]\n",
    "len(image_list) / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(images, rows=8, cols=8):\n",
    "    grid = np.zeros([images.shape[1] * rows, images.shape[2] * cols, images.shape[3]])\n",
    "    \n",
    "    if rows * cols > images.shape[0]:\n",
    "        return\n",
    "    \n",
    "    i = 0\n",
    "    for y in range(0, grid.shape[0], images.shape[1]):\n",
    "        for x in range(0, grid.shape[1], images.shape[2]):\n",
    "            grid[y:y+images.shape[1], x:x + images.shape[2]] = images[i]\n",
    "            i += 1\n",
    "    return grid\n",
    "\n",
    "def plot_images(images, rows=8, cols=8):\n",
    "    grid = make_grid(images, rows, cols)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(np.squeeze(grid), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(image):\n",
    "    return ((image + 1.) / 2.).clip(0, 1)\n",
    "\n",
    "def transform(image):\n",
    "    return ((image * 2. - 1.)).clip(-1, 1)\n",
    "\n",
    "def conv_out_size_same(size, stride):\n",
    "    return int(math.ceil(float(size) / float(stride)))\n",
    "\n",
    "def conv2d(x, out_channels, name='conv2d', kernel_h=5, kernel_w=5, \n",
    "                     stride_height=2, stride_width=2, padding='SAME', bias=True, stddev=0.02):\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.truncated_normal([kernel_h, kernel_w, tf.shape(x)[3], out_channels], stddev=stddev)\n",
    "\n",
    "        conv_layer = tf.nn.conv2d(x, W, strides=[1, stride_height, stride_width, 1], \n",
    "                                  padding=padding)\n",
    "        if bias:\n",
    "            biases = tf.get_variable('biases', [out_channels], \n",
    "                                     initializer=tf.constant_initializer(0.0))\n",
    "            return tf.nn.bias_add(conv_layer, biases)\n",
    "        else:\n",
    "            return conv_layer\n",
    "\n",
    "def transpose_conv2d(x, output_shape, name='transpose_conv2d', kernel_h=5, kernel_w=5,\n",
    "                     stride_height=2, stride_width=2, padding='SAME', stddev=0.02):\n",
    "    with tf.variable_scope(name):\n",
    "        # [height, width, out_channels, in_channels]\n",
    "        W = tf.truncated_normal([kernel_h, kernel_w, output_shape[3], tf.shape(x)[3]], stddev=stddev)\n",
    "        transpose_conv = tf.nn.conv2d_transpose(x, W, output_shape=output_shape, \n",
    "                                                strides=[1, stride_height, stride_width, 1])\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape[3]], initializer=tf.constant_initializer(0.0))\n",
    "        return tf.nn.bias_add(transpose_conv, biases)\n",
    "\n",
    "\n",
    "def generator(z, filters=[256, 128, 64], kernels=[5, 5, 5], \n",
    "              strides=[2, 2, 2], training=True, reuse=False, use_batchnorm=True):\n",
    "    s_h, s_w = HEIGHT, WIDTH\n",
    "    for i in range(len(filters)):\n",
    "        s_h, s_w = conv_out_size_same(s_h, strides[i]), conv_out_size_same(s_w, strides[i])\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse) as scope:\n",
    "        z = tf.layers.dense(z, filters[0] * s_h * s_w, use_bias=False, name='g_h0')\n",
    "        z = tf.reshape(z, [-1, s_h, s_w, filters[0]])\n",
    "        z = tf.layers.batch_normalization(z, training=training, name='g_bn0')\n",
    "        z = tf.nn.relu(z)\n",
    "\n",
    "        for i in range(len(filters) - 1):\n",
    "            z = tf.layers.conv2d_transpose(z, filters[i + 1], kernel_size=kernels[i], strides=strides[i], padding='SAME',\n",
    "                                            kernel_initializer=tf.initializers.truncated_normal(mean=0, stddev=0.02),\n",
    "                                            name=f'g_h{i + 1}')\n",
    "            if use_batchnorm:\n",
    "                z = tf.layers.batch_normalization(z, training=training, name=f'g_bn{i + 1}')\n",
    "            z = tf.nn.relu(z)\n",
    "        \n",
    "\n",
    "        z = tf.layers.conv2d_transpose(z, CHANNELS, kernel_size=kernels[len(filters) - 1], \n",
    "                                       strides=strides[len(filters) - 1], padding='SAME',\n",
    "                                        kernel_initializer=tf.initializers.truncated_normal(mean=0, stddev=0.02),\n",
    "                                        name=f'g_h{len(filters)}')        \n",
    "        return tf.nn.tanh(z)\n",
    "\n",
    "def discriminator(x, filters=[64, 128, 256], kernels=[5, 5, 5], \n",
    "                  strides=[2, 2, 2], training=True, reuse=False, use_batchnorm=True):\n",
    "    s_h, s_w = HEIGHT, WIDTH\n",
    "    for i in range(len(filters)):\n",
    "        s_h, s_w = conv_out_size_same(s_h, strides[i]), conv_out_size_same(s_w, strides[i])\n",
    "\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse) as scope:\n",
    "        for i in range(len(filters)):\n",
    "            x = tf.layers.conv2d(x, filters[i], kernel_size=kernels[i], strides=strides[i], padding='SAME',\n",
    "                                   kernel_initializer=tf.initializers.truncated_normal(mean=0, stddev=0.02),\n",
    "                                   name=f'd_h{i}')\n",
    "            if use_batchnorm:\n",
    "                x = tf.layers.batch_normalization(x, training=training, name=f'd_bn{i}')\n",
    "\n",
    "            x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "\n",
    "        x = tf.layers.dense(tf.reshape(x, [BATCH_SIZE, filters[-1] * s_h * s_w]), 1, name=f'd_h{len(filters)}')\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    z = tf.placeholder(tf.float32, shape=[None, Z_DIM])\n",
    "    z_fixed = tf.placeholder(tf.float32, shape=[None, Z_DIM])\n",
    "    x = tf.placeholder(tf.float32, shape=[None, HEIGHT, WIDTH, CHANNELS])\n",
    "    \n",
    "    \n",
    "    g = generator(z, g_filters, g_kernels, g_strides, \n",
    "                  training=True, reuse=False, use_batchnorm=G_USE_BATCHNORM)\n",
    "    \n",
    "    g_fixed = generator(z_fixed, g_filters, g_kernels, g_strides, \n",
    "                        training=False, reuse=True,  use_batchnorm=G_USE_BATCHNORM)\n",
    "    \n",
    "    real_logits = discriminator(x, d_filters, d_kernels, d_strides, \n",
    "                                training=True, reuse=False, use_batchnorm=D_USE_BATCHNORM)\n",
    "    \n",
    "    fake_logits = discriminator(g, d_filters, d_kernels, d_strides, \n",
    "                                training=True, reuse=True, use_batchnorm=D_USE_BATCHNORM)\n",
    "    \n",
    "    w_distance = tf.reduce_mean(real_logits) - tf.reduce_mean(fake_logits)\n",
    "    d_loss = -w_distance\n",
    "    g_loss = -tf.reduce_mean(fake_logits)\n",
    "    \n",
    "    w_distance_summary = tf.summary.scalar(\"w_distance\", w_distance)\n",
    "    d_loss_summary = tf.summary.scalar(\"d_loss\", d_loss)\n",
    "    g_loss_summary = tf.summary.scalar(\"g_loss\", g_loss)\n",
    "\n",
    "    images_summary = tf.summary.image(\"generated_images\", (g_fixed + 1.) / 2.)\n",
    "    trainable_vars = tf.trainable_variables()\n",
    "    \n",
    "    trainable_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in trainable_vars if 'discriminator' in var.name]\n",
    "    g_vars = [var for var in trainable_vars if 'generator' in var.name]\n",
    "    \n",
    "    # Loss: Wasserstein loss. REquires clipping WEIGHTS to ensure LIPSCHITZ CONSTRAINTS\n",
    "    d_clip = [p.assign(tf.clip_by_value(p, -WEIGHT_CLIP, WEIGHT_CLIP)) for p in d_vars]\n",
    "\n",
    "    d_optim = tf.train.AdamOptimizer(D_LEARNING_RATE, beta1=BETA1)\\\n",
    "                    .minimize(d_loss, var_list=d_vars)\n",
    "    g_optim = tf.train.AdamOptimizer(G_LEARNING_RATE, beta1=BETA1)\\\n",
    "                    .minimize(g_loss, var_list=g_vars)\n",
    "    \n",
    "        \n",
    "    d_summary = tf.summary.merge([d_loss_summary, w_distance_summary])\n",
    "    g_summary = tf.summary.merge([g_loss_summary, images_summary])\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter(\"./runs/{}\".format(RUN_NAME), sess.graph)\n",
    "    summary_step = 0\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    z_fixed_input = np.random.uniform(-1, 1, [BATCH_SIZE, Z_DIM])\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        print(\"Beginning Training...\")\n",
    "        np.random.shuffle(image_list)\n",
    "        i = 0\n",
    "        while i < len(image_list):\n",
    "            x_input = np.zeros([BATCH_SIZE, HEIGHT, WIDTH, CHANNELS])\n",
    "            for k in range(0, BATCH_SIZE):\n",
    "                x_input[k] = cv2.imread(os.path.join(image_dir, image_list[i + k]))[:,:,[0]]\n",
    "            x_input = transform(x_input / 255)\n",
    "            z_input = np.random.uniform(-1, 1, [BATCH_SIZE, Z_DIM])\n",
    "            \n",
    "            sess.run(d_clip)\n",
    "            [_, d_summary_result, d_loss_val] = sess.run([d_optim, d_summary, d_loss], \n",
    "                                                         feed_dict={z: z_input, x: x_input})\n",
    "            \n",
    "            for _ in range(G_ITERS):\n",
    "                [_, g_img, g_summary_result] = sess.run([g_optim, g_fixed, g_summary], feed_dict={z: z_input,\n",
    "                                                                                                   z_fixed: z_fixed_input,\n",
    "                                                                                                   x: x_input})                \n",
    "            if (i % (BATCH_SIZE * 50) == 0):\n",
    "                writer.add_summary(g_summary_result, summary_step)\n",
    "\n",
    "                writer.add_summary(d_summary_result, summary_step)\n",
    "                summary_step += 1\n",
    "            \n",
    "            i += BATCH_SIZE\n",
    "       \n",
    "        plot_images(inverse_transform(g_img))\n",
    "\n",
    "        save_path = saver.save(sess, \"./checkpoints/{}\".format(RUN_NAME))\n",
    "        print(f\"[Epoch {epoch}/{N_EPOCHS}] -- Saved {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
