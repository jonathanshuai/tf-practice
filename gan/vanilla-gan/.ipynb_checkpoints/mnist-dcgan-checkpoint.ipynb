{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15009616990453215974\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6709723791\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10648463595186186458\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:08:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CROPS = False # Determine whether to run crops or not\n",
    "\n",
    "HEIGHT = 28 # Height of the image\n",
    "WIDTH = 28 # Width of the image\n",
    "CHANNELS = 1 # Number of channels for an image\n",
    "\n",
    "Z_DIM = 100 # Dimensions of noise vector z\n",
    "BATCH_SIZE = 64 # Batch size for training\n",
    "\n",
    "G_FILTERS = 32 # Number of filters in the final deconv layer for Generator\n",
    "D_FILTERS = 32 # Number of filters in the first conv layer for Discriminator\n",
    "\n",
    "D_LEARNING_RATE = 1.5e-4\n",
    "G_LEARNING_RATE = 1.5e-4\n",
    "N_EPOCHS = 50\n",
    "BETA1 = 0.5\n",
    "G_ITERS = 4\n",
    "G_USE_BATCHNORM = True\n",
    "D_USE_BATCHNORM = True\n",
    "TIME_FORMAT = datetime.now().strftime('%h-%d-%Y-%Hh%Mm%Ss')\n",
    "RUN_NAME = f\"d_lr_{D_LEARNING_RATE}_g_lr{G_LEARNING_RATE}_{BETA1}_{G_ITERS}_{N_EPOCHS}_dbn_{D_USE_BATCHNORM}_gbn_{G_USE_BATCHNORM}_{TIME_FORMAT}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_dir = 'C:\\\\Users\\\\Jonathan\\\\Documents\\\\development\\\\datasets\\\\images\\\\mnist\\\\training'\n",
    "image_list = []\n",
    "for directory in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "    for root, d, files in os.walk(os.path.join(image_dir, directory)):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):\n",
    "                image_list.append(os.path.join(directory, file))\n",
    "\n",
    "plt.imshow(cv2.imread(os.path.join(image_dir, image_list[0])))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "936.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list = image_list[:59904]\n",
    "len(image_list) / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(images, rows=8, cols=8):\n",
    "    grid = np.zeros([images.shape[1] * rows, images.shape[2] * cols, images.shape[3]])\n",
    "    \n",
    "    if rows * cols > images.shape[0]:\n",
    "        return\n",
    "    \n",
    "    i = 0\n",
    "    for y in range(0, grid.shape[0], images.shape[1]):\n",
    "        for x in range(0, grid.shape[1], images.shape[2]):\n",
    "            grid[y:y+images.shape[1], x:x + images.shape[2]] = images[i]\n",
    "            i += 1\n",
    "    return grid\n",
    "\n",
    "def plot_images(images, rows=8, cols=8):\n",
    "    grid = make_grid(images, rows, cols)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(np.squeeze(grid), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(image):\n",
    "    return ((image + 1.) / 2.).clip(0, 1)\n",
    "\n",
    "def transform(image):\n",
    "    return ((image * 2. - 1.)).clip(-1, 1)\n",
    "\n",
    "def conv_out_size_same(size, stride):\n",
    "    return int(math.ceil(float(size) / float(stride)))\n",
    "\n",
    "def conv2d(x, out_channels, name='conv2d', kernel_h=5, kernel_w=5, \n",
    "                     stride_height=2, stride_width=2, padding='SAME', bias=True, stddev=0.02):\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.truncated_normal([kernel_h, kernel_w, tf.shape(x)[3], out_channels], stddev=stddev)\n",
    "\n",
    "        conv_layer = tf.nn.conv2d(x, W, strides=[1, stride_height, stride_width, 1], \n",
    "                                  padding=padding)\n",
    "        if bias:\n",
    "            biases = tf.get_variable('biases', [out_channels], \n",
    "                                     initializer=tf.constant_initializer(0.0))\n",
    "            return tf.nn.bias_add(conv_layer, biases)\n",
    "        else:\n",
    "            return conv_layer\n",
    "\n",
    "def transpose_conv2d(x, output_shape, name='transpose_conv2d', kernel_h=5, kernel_w=5,\n",
    "                     stride_height=2, stride_width=2, padding='SAME', stddev=0.02):\n",
    "    with tf.variable_scope(name):\n",
    "        # [height, width, out_channels, in_channels]\n",
    "        W = tf.truncated_normal([kernel_h, kernel_w, output_shape[3], tf.shape(x)[3]], stddev=stddev)\n",
    "        transpose_conv = tf.nn.conv2d_transpose(x, W, output_shape=output_shape, \n",
    "                                                strides=[1, stride_height, stride_width, 1])\n",
    "\n",
    "        biases = tf.get_variable('biases', [output_shape[3]], initializer=tf.constant_initializer(0.0))\n",
    "        return tf.nn.bias_add(transpose_conv, biases)\n",
    "\n",
    "\n",
    "def generator(z, filters=[256, 128, 64], kernels=[5, 5, 5], \n",
    "              strides=[2, 2, 2], training=True, reuse=False, use_batchnorm=True):\n",
    "    s_h, s_w = HEIGHT, WIDTH\n",
    "    for i in range(len(filters)):\n",
    "        s_h, s_w = conv_out_size_same(s_h, strides[i]), conv_out_size_same(s_w, strides[i])\n",
    "    \n",
    "    with tf.variable_scope(\"generator\", reuse=reuse) as scope:\n",
    "        z = tf.layers.dense(z, filters[0] * s_h * s_w, use_bias=False, name='g_h0')\n",
    "        z = tf.reshape(z, [-1, s_h, s_w, filters[0]])\n",
    "        z = tf.layers.batch_normalization(z, training=training, name='g_bn0')\n",
    "        z = tf.nn.relu(z)\n",
    "\n",
    "        for i in range(len(filters) - 1):\n",
    "            z = tf.layers.conv2d_transpose(z, filters[i + 1], kernel_size=kernels[i], strides=strides[i], padding='SAME',\n",
    "                                            kernel_initializer=tf.initializers.truncated_normal(mean=0, stddev=0.02),\n",
    "                                            name=f'g_h{i + 1}')\n",
    "            if use_batchnorm:\n",
    "                z = tf.layers.batch_normalization(z, training=training, name=f'g_bn{i + 1}')\n",
    "            z = tf.nn.relu(z)\n",
    "        \n",
    "\n",
    "        z = tf.layers.conv2d_transpose(z, CHANNELS, kernel_size=kernels[len(filters) - 1], \n",
    "                                       strides=strides[len(filters) - 1], padding='SAME',\n",
    "                                        kernel_initializer=tf.initializers.truncated_normal(mean=0, stddev=0.02),\n",
    "                                        name=f'g_h{len(filters)}')        \n",
    "        return tf.nn.tanh(z)\n",
    "\n",
    "def discriminator(x, filters=[64, 128, 256], kernels=[5, 5, 5], \n",
    "                  strides=[2, 2, 2], training=True, reuse=False, use_batchnorm=True):\n",
    "    s_h, s_w = HEIGHT, WIDTH\n",
    "    for i in range(len(filters)):\n",
    "        s_h, s_w = conv_out_size_same(s_h, strides[i]), conv_out_size_same(s_w, strides[i])\n",
    "\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse) as scope:\n",
    "        for i in range(len(filters)):\n",
    "            x = tf.layers.conv2d(x, filters[i], kernel_size=kernels[i], strides=strides[i], padding='SAME',\n",
    "                                   kernel_initializer=tf.initializers.truncated_normal(mean=0, stddev=0.02),\n",
    "                                   name=f'd_h{i}')\n",
    "            if use_batchnorm:\n",
    "                x = tf.layers.batch_normalization(x, training=training, name=f'd_bn{i}')\n",
    "\n",
    "            x = tf.nn.leaky_relu(x, alpha=0.2)\n",
    "\n",
    "        x = tf.layers.dense(tf.reshape(x, [BATCH_SIZE, filters[-1] * s_h * s_w]), 1, name=f'd_h{len(filters)}')\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_filters = [256, 128, 64]\n",
    "g_kernels = [5, 5, 5]\n",
    "g_strides = [2, 1, 1]\n",
    "\n",
    "d_filters = [64, 128, 256]\n",
    "d_kernels = [5, 5, 5]\n",
    "d_strides = [2, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'discriminator/d_h0/kernel:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'discriminator/d_h0/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'discriminator/d_bn0/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'discriminator/d_bn0/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'discriminator/d_h1/kernel:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'discriminator/d_h1/bias:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'discriminator/d_bn1/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'discriminator/d_bn1/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'discriminator/d_h2/kernel:0' shape=(5, 5, 128, 256) dtype=float32_ref>, <tf.Variable 'discriminator/d_h2/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'discriminator/d_bn2/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'discriminator/d_bn2/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'discriminator/d_h3/kernel:0' shape=(50176, 1) dtype=float32_ref>, <tf.Variable 'discriminator/d_h3/bias:0' shape=(1,) dtype=float32_ref>]\n",
      "[<tf.Variable 'generator/g_h0/kernel:0' shape=(100, 50176) dtype=float32_ref>, <tf.Variable 'generator/g_bn0/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'generator/g_bn0/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'generator/g_h1/kernel:0' shape=(5, 5, 128, 256) dtype=float32_ref>, <tf.Variable 'generator/g_h1/bias:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'generator/g_bn1/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'generator/g_bn1/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'generator/g_h2/kernel:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'generator/g_h2/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'generator/g_bn2/gamma:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'generator/g_bn2/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'generator/g_h3/kernel:0' shape=(5, 5, 1, 64) dtype=float32_ref>, <tf.Variable 'generator/g_h3/bias:0' shape=(1,) dtype=float32_ref>]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] -- Saved ./checkpoints/d_lr_0.00015_g_lr0.00015_0.5_4_50_dbn_True_gbn_True_Mar-14-2019-22h12m42s\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    z = tf.placeholder(tf.float32, shape=[None, Z_DIM])\n",
    "    z_fixed = tf.placeholder(tf.float32, shape=[None, Z_DIM])\n",
    "    x = tf.placeholder(tf.float32, shape=[None, HEIGHT, WIDTH, CHANNELS])\n",
    "    \n",
    "    \n",
    "    g = generator(z, g_filters, g_kernels, g_strides, \n",
    "                  training=True, reuse=False, use_batchnorm=G_USE_BATCHNORM)\n",
    "    g_fixed = generator(z_fixed, g_filters, g_kernels, g_strides, \n",
    "                        training=False, reuse=True,  use_batchnorm=G_USE_BATCHNORM)\n",
    "    real_logits = discriminator(x, d_filters, d_kernels, d_strides, \n",
    "                                training=True, reuse=False, use_batchnorm=D_USE_BATCHNORM)\n",
    "    fake_logits = discriminator(g, d_filters, d_kernels, d_strides, \n",
    "                                training=True, reuse=True, use_batchnorm=D_USE_BATCHNORM)\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, \n",
    "                                                                         labels=tf.ones_like(real_logits)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, \n",
    "                                                                         labels=tf.zeros_like(fake_logits)))\n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, \n",
    "                                                                    labels=tf.ones_like(fake_logits)))\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    d_loss_real_summary = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
    "    d_loss_fake_summary = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
    "    g_loss_summary = tf.summary.scalar(\"g_loss\", g_loss)\n",
    "    d_loss_summary = tf.summary.scalar(\"d_loss\", d_loss)\n",
    "\n",
    "    images_summary = tf.summary.image(\"generated_images\", (g_fixed + 1.) / 2.)\n",
    "    trainable_vars = tf.trainable_variables()\n",
    "    \n",
    "    trainable_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in trainable_vars if 'discriminator' in var.name]\n",
    "    g_vars = [var for var in trainable_vars if 'generator' in var.name]\n",
    "    \n",
    "    print(d_vars)\n",
    "    print(g_vars)\n",
    "    \n",
    "    d_optim = tf.train.AdamOptimizer(D_LEARNING_RATE, beta1=BETA1)\\\n",
    "                    .minimize(d_loss, var_list=d_vars)\n",
    "    g_optim = tf.train.AdamOptimizer(G_LEARNING_RATE, beta1=BETA1)\\\n",
    "                    .minimize(g_loss, var_list=g_vars)\n",
    "    \n",
    "    d_summary = tf.summary.merge([d_loss_real_summary, d_loss_fake_summary, d_loss_summary])\n",
    "    g_summary = tf.summary.merge([g_loss_summary, images_summary])\n",
    "    \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter(\"./runs/{}\".format(RUN_NAME), sess.graph)\n",
    "    summary_step = 0\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    z_fixed_input = np.random.uniform(-1, 1, [BATCH_SIZE, Z_DIM])\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        np.random.shuffle(image_list)\n",
    "        i = 0\n",
    "        while i < len(image_list):\n",
    "            x_input = np.zeros([BATCH_SIZE, HEIGHT, WIDTH, CHANNELS])\n",
    "            for k in range(0, BATCH_SIZE):\n",
    "                x_input[k] = cv2.imread(os.path.join(image_dir, image_list[i + k]))[:,:,[0]]\n",
    "            x_input = transform(x_input / 255)\n",
    "            z_input = np.random.uniform(-0.1, 0.1, [BATCH_SIZE, Z_DIM])\n",
    "            \n",
    "            \n",
    "            [_, d_summary_result, d_loss_val, dlrv, dlfv] = sess.run([d_optim, d_summary, d_loss, d_loss_real, d_loss_fake], feed_dict={z: z_input, x: x_input})\n",
    "            \n",
    "            for _ in range(G_ITERS):\n",
    "                [_, g_img, g_summary_result] = sess.run([g_optim, g_fixed, g_summary], feed_dict={z: z_input,\n",
    "                                                                                                   z_fixed: z_fixed_input,\n",
    "                                                                                                   x: x_input})                \n",
    "            if (i % (BATCH_SIZE * 50) == 0):\n",
    "                writer.add_summary(g_summary_result, summary_step)\n",
    "\n",
    "                writer.add_summary(d_summary_result, summary_step)\n",
    "                summary_step += 1\n",
    "            \n",
    "            i += BATCH_SIZE\n",
    "       \n",
    "        plot_images(inverse_transform(g_img))\n",
    "\n",
    "        save_path = saver.save(sess, \"./checkpoints/{}\".format(RUN_NAME))\n",
    "        print(f\"[Epoch {epoch}/{N_EPOCHS}] -- Saved {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_images = np.squeeze(make_grid(inverse_transform(g_img)))\n",
    "# plt.imshow(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(inverse_transform(x_input) > -0.01).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(make_grid(inverse_transform(x_input))), cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
